# Alok Singh

| <alokbeniwal@gmail.com> â€¢ 01 (408) 421-5658
| 2042 Hearst Ave Apt C, Berkeley, CA 94709
| [alok.blog](https://alok.github.io/)
| [github.com/alok](https://www.github.com/alok/)

## Education and Coursework

2013-2017

:   **BA, Mathematics**; University of California, Berkeley

    Deep Reinforcement Learning, Machine Learning, Functional Analysis,
    Real Analysis, Topology, Measure Theory, Artificial Intelligence,
    Algebraic Topology

## Relevant Experience

**Machine Learning Consultant, Papert Labs** (2018)

Consulted companies on how to use ML.

Primarily worked on time series forecasting and automating the ML
pipeline. Cut time to run pipeline by 2 hours.

**Recurse Center** (2017)

Worked on deep reinforcement learning. Implemented DAgger, DQN, and
policy gradient methods.

Also worked on metaheuristics such as genetic algorithms and simulated
annealing.

**Data Scientist Intern, Radius Intelligence** (2015)

-   Did data cleaning and exploratory data analysis on customer-provided
    data using Apache Spark
-   Found a security vulnerability relating to customer passwords and
    had it fixed.
-   Integrated customer data with our own, increasing total dataset size
    by 3x.

## Relevant Projects

**Network Compression**

Implemented model compression to test the conclusion of the paper
*Understanding Deep Learning Requires Rethinking Generalization*. Blog
post and code
[here](https://alok.github.io/2018/01/12/compressing-neural-networks-to-see-if-they-learn).

**Deep Q-Network**

Implemented DQN to play Atari games (Pong and Breakout). Also
reimplemented from scratch for basic environments such as CartPole.

**DAgger**

Implemented and gave a talk about DAgger at the Recurse Center.

**A2C**

Implemented A2C to solve simple continuous control problems.

**Genetic Algorithm to optimize hyperparameters for neural nets**

Found optimal hyperparameters for neural network architecture 10x faster
than brute search.

## Skills

-   Python, PyTorch, TensorFlow, shell scripting, Haskell
