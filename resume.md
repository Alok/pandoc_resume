# Alok Singh

| <alokbeniwal@gmail.com> â€¢ 01 (408) 421-5658
| 2042 Hearst Ave Apt C, Berkeley, CA 94709
| [alok.blog](https://alok.github.io/)
| [github.com/alok](https://www.github.com/alok/)

## Education and Coursework

2013-2017

:   **BA, Mathematics**; University of California, Berkeley

    Deep Reinforcement Learning (CS 294), Machine Learning (CS 189),
    Functional Analysis, Topology and Measure Theory, Artificial
    Intelligence (CS 188), Algebraic Topology

## Relevant Experience

**Resident, Recurse Center** (2017)

Worked on deep reinforcement learning. Implemented DAgger, DQN, and
policy gradient methods.

Also worked on metaheuristics such as genetic algorithms and simulated
annealing.

**Data Scientist Intern, Radius Intelligence (2015)**

-   Did a lot of data cleaning
-   Found a security vulnerability and had it fixed.
-   Integrated customer data with our own, increasing total dataset size
    by 3x.

## Relevant Projects

**Network Compression**

Implemented model compression to test the conclusion of the paper
*Understanding Deep Learning Requires Rethinking Generalization*. Blog
post and code
[here](https://alok.github.io/2018/01/12/compressing-neural-networks-to-see-if-they-learn).

**Deep Q-Network**

Implemented DQN to play Atari games (Pong and Breakout). Also
reimplemented from scratch for basic OpenAI Gym environments.

**DAgger**

Implemented and gave a talk about DAgger at the Recurse Center.

**Policy Gradient and Actor Critic**

Implemented both.

**Genetic Algorithm to optimize hyperparameters for neural nets**

Found optimal parameters for architecture 10x faster than brute search.

**Phylogenetic Tree Construction**

Built a phylogenetic tree using a modified Jukes-Cantor method.

**Tomographic Reconstruction**

Reconstructed 3D images of protein tertiary structure from 2D
projections by applying Fourier analysis.

## Blog Posts

**MAML**

Sketched out MAML
[here](https://alok.github.io/2018/01/16/my-morning-straitjacket-model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks-maml).

**Vanishing/Exploding Gradients and Power Iteration**

[How eigenvalue analysis explains why power iteration works and why
gradients
explode](https://alok.github.io/2017/09/18/eigenapplication-explosions).

## Skills

-   Python, PyTorch, TensorFlow, Keras, Bash, Haskell, Apache Spark
